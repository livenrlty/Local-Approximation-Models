{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pd \n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/segments.pickle\", \"rb\") as f:\n",
    "    segments_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_segment(segment, output_length):\n",
    "    segment_indices = np.arange(segment.size)\n",
    "    interpolated_f = interp1d(segment_indices,\n",
    "                                                segment,\n",
    "                                                kind='cubic')\n",
    "    new_indices = np.linspace(0, segment_indices[-1], output_length)\n",
    "    return interpolated_f(new_indices)\n",
    "\n",
    "\n",
    "def normalize_segments(segments, length=None):\n",
    "    segments = list(segments)\n",
    "    length = length if length else min(segment.size for segment in segments)\n",
    "    return [shrink_segment(segment, length) for segment in segments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_and_segment = []\n",
    "\n",
    "for key, segments_array in segments_dict.items():\n",
    "    action_and_segment.extend([(key, s) for s in segments_array if s.size > 30])\n",
    "\n",
    "segments = [pair[1] for pair in action_and_segment]\n",
    "actions = [pair[0] for pair in action_and_segment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_segments = normalize_segments(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_and_norm_segment = [(pair[0][0], pair[1])\n",
    "                           for pair in zip(action_and_segment, normalized_segments)]\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(actions)\n",
    "X = np.array(normalized_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 215)\n",
    "np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[a]\n",
    "Y = Y[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "accs = []\n",
    "\n",
    "for train_idx, test_idx in kfold.split(X, Y):\n",
    "    x_train, x_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=(60), max_iter=1500, activation='logistic')\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)\n",
    "    scores.append(sklearn.metrics.log_loss(y_test, y_pred))\n",
    "    accs.append(np.sum(y_test == model.predict(x_test))/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7674418604651163,\n",
       " 0.8372093023255814,\n",
       " 0.8604651162790697,\n",
       " 0.8372093023255814,\n",
       " 0.9069767441860465]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-e81e014786c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0m__supported_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_mem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-239-e81e014786c9>\u001b[0m in \u001b[0;36mSSA\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m__supported_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_mem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "class SSA(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr)\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
